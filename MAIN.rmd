---
title: "Project"
output: html_document
---

```{r include=FALSE}
# settings
knitr::opts_chunk$set(echo = TRUE)

```

קריאה לספריות העיקריות וקביעת סיד

```{r eval=TRUE, include=FALSE}
library(tidyverse)
#install.packages("gtrendsR")
library(gtrendsR)
#library("docstring")
source("project_utils.R")
SEED = 123
```

```{=html}
<style>
    p  {direction: rtl;}
    h1 {text-align: center;}
    h2 {direction: rtl;}
    h4 {direction: rtl;}
    h3 {direction: rtl;text-decoration: underline;}
    ol {direction: rtl;}
    li {direction: rtl;}
</style>
```
::: {style="border-bottom:solid"}
:::

קריאה למאגרי נתונים שלנו

```{r getting data}
fmt <- "%Y-%m-%d"

data <- get_data() # the main dataset, contains ads info
rating <- get_rating_data() # contains rating info, e.g., ads cost, viewrs and etc
df <- read.csv("data\\trends_data.csv")
brands = unique(data['brand'])
```

## תוצאות

## התאמה ליניארית

כאן אנחנו יוצרים את מודל הריבועים הפחותים ואת הקו החסין ןגם מחשבים את מקדם המתאם של פירסון של המודל הלינארי, בנוסף אנו יוצרים תרשים של המודל ותרשים שאריות נפרד לכל גרף

```{r message=FALSE, warning=FALSE}

#generating the least squares linear model in R
le_model <- lm(ratio~ad_cost,data = z)
summary(le_model)

#finding Pearson's coefficient
(Pearsons_coeff <- cor( x= z$ad_cost,y = z$ratio))

#calculating robust coefficients
robust_coef<- TukeyLine(z$ad_cost,z$ratio)

linear.model.plot(z$ad_cost,z$ratio,le_model,robust_coef)

#least squares residual plot
plot(x =z$ad_cost,y = le_model$residuals,main = "least squares model residuals plot",xlab = "Ad cost",ylab = "residuals",las=1)
abline(h=0,col="red")

#robust line residual plot
robust_resid <- z$ratio-(robust_coef[1]+robust_coef[2]*z$ad_cost)
plot(x =z$ad_cost,y =robust_resid ,main = "robust line model residuals plot",xlab = "Ad cost",ylab = "residuals",las=1)
abline(h=0,col="red")


```

כאן אנחנו יוצרים מודלים לינארי עם טרנספורמציית לוג על המשתנה המסביר, מחשבים את מקדם פירסון עבור המודל עם הטרנספורמציה ויוצרים תרשימי שאריות עבור הקו החסין וקו הריבועים הפחותים

```{r}
#generating the log model
le_model_log <- lm(ratio~log(ad_cost),data = z)
summary(le_model_log)
(Pearsons_coeff_log <- cor(x = log(z$ad_cost),y = z$ratio))

#logarithm transformation models plot
robust_coef_log<- TukeyLine(log(z$ad_cost),z$ratio)
plot(y = z$ratio,x = log(z$ad_cost),main = "Search ratio VS log(Ads cost)",xlab = "Ad cost",ylab = "Search ratio",las=1)
abline(a=coefficients(le_model_log)[1],b =coefficients(le_model_log)[2],col="red")
abline(a=robust_coef_log[1],b=robust_coef_log[2],col="purple")
legend("topright",legend = c("Least squares","Robust line"),col = c("red","purple"),lty =1 )

#log transformation residuals
plot(x =log(z$ad_cost),y = le_model_log$residuals,main = "least squares log model residuals plot",xlab = "Ad cost",ylab = "residuals",las=1)
abline(h=0,col="red")
robust_resid_log <- z$ratio-(robust_coef_log[1]+robust_coef_log[2]*log(z$ad_cost))
plot(x =log(z$ad_cost),y =robust_resid_log ,main = "robust line log model residuals plot",xlab = "Ad cost",ylab = "residuals",las=1)
abline(h=0,col="red")

```

### מבחן 1

בדיקה האם יש קשר בין השבוע עם היטס 100 לשבוע הסופרבול כלומר אנחנו בודקים אם יש קשר בין השבוע בשנה שהיו בו הכי הרבה חיפושים לשבוע הסופרבול

איך הגענו לטבלת השכיחות

```{r message=TRUE, warning=TRUE}
hits_on_sup <- filter(df, is_sup_week==TRUE)$hits
hits_not_on_sup <- filter(df, is_sup_week==FALSE)$hits

freq_table  <- df %>% group_by(is_peak=hits==100, is_sup_week) %>% summarise(count=n())
freq_table
```

בניית טבלת השכיחות וביצוע המבחן

```{r}
library("USP")

freq <- cbind(c(sum(hits_on_sup == 100), sum(hits_on_sup != 100)),
      c(sum(hits_not_on_sup == 100), sum(hits_not_on_sup != 100)))

freq
result <- USP.test(freq, B = 9999)
result$p.value
result$TestStat
```

### מבחן 2

אנחנו מבצעים אמידה ברווח בר סמך לתוחלת של ההיטס בסופרבול ושל ההיטס לא בסופרבול

```{r}
set.seed(SEED)
t.test(hits_on_sup)$conf.int
samp <- sample(hits_not_on_sup, 100)
t.test(samp)$conf.int

# To show that our sample is roughly normally distributed 

qqnorm(samp)
qqline(samp)

```

### מבחן 2

כאן אנחנו בודקים את הפרש התוחלות בין ההיטס בשבוע הסופרבול בין ההיטס בשבוע שהוא לא שבוע הסופרבול

```{r warning=FALSE}
set.seed(SEED)

samp <- sample(hits_not_on_sup, 300)
t.test(hits_on_sup, samp, alternative = "greater", paired = FALSE,var.equal = 0, mu=25)

# Show that the differences are normally distributed
d <- hits_on_sup - samp
qqnorm(d)
qqline(d)
```

### מבחן 4

מבחן מזווג של ממוצע היטס בשנה שבו חברה כן פרסמה לעומת שבה היא לא פרסמה

נבנה דאטהפריימ מתאים

```{r message=FALSE, warning=FALSE}

on_sup <- tibble(year=c(as.numeric(NA)),
               brand=c(as.character(NA)),
               hits=c(as.numeric(NA))) %>% na.omit()

not_sup <- tibble(year=c(as.numeric(NA)),
               brand=c(as.character(NA)),
               hits=c(as.numeric(NA))) %>% na.omit()


for (target_brand in brands$brand) 
  {
  
      sup_years <- year(search_dates(target_brand)$date)
      not_sup_years <- setdiff(2004:2020,year(search_dates(target_brand)$date))
      
      n <- min(length(sup_years), length(not_sup_years))
      # the yearly mean hits value
      means <- filter(df, brand==target_brand) %>% 
        group_by(brand, year=year(date)) %>%
        summarise(hits=mean(hits)) %>% ungroup()
      
      # Here we ensure that on_sup receives only superbowl years values
      # and similarly about not sup 
      on_sup <- on_sup %>% add_row(means[sup_years[1:n]- 2004 + 1,]) 
      not_sup <- not_sup %>% add_row(means[not_sup_years[1:n] - 2004 + 1,])
     
      
}


```

מהתרשים ההפרשים בהתאם להנחות עבר שלנו (דגימות משנים קודמות בלתי תלויות) אפשר לראות כי ההפרשים מתפלגים בקירוב נורמלי ובמבחן המזווג לא נדחה את השארת האפס

```{r message=FALSE, warning=FALSE}

t.test(on_sup$hits,not_sup$hits, paired = TRUE)

d <- on_sup$hits - not_sup$hits
qqnorm(d)
qqline(d)
```

## נספחים-גרפים

```{r}
by_week <- df %>% 
  group_by(is_sup_week, brand) %>% 
  summarise(mean_hits=mean(hits))

ggplot(data=by_week, aes(x=brand, y=mean_hits, fill=as.character(is_sup_week), width=0.8)) +
  geom_bar(stat="identity", position=position_dodge())+
  labs(title="גרף ערכי החיפוש השבועי הממוצע בשבוע הסופרבול לעומת שבוע לא סופרבול", x="חברה", y="  שבועי ממוצע hits ערך", fill="שבוע סופרבול") +
  scale_fill_brewer(palette="Paired")+
  theme_minimal()
```

+סימולציה להנחת הנורמליות

תרשים שאריות שמראים שהממוצע ההיטס אכן מתפלג נורמלית

```{r}
n = 5000
samp_size <- 50
samples <- c()
for (i in 1:n) {samples[i] = mean(sample(hits_on_sup, samp_size))}
hist(samples)
qqnorm(samples)
qqline(samples)
mean(samples)
```
